name: "process_task_results"
namespace: "reporting"
version: "build_main"
argument_groups:
- name: "Inputs"
  arguments:
  - type: "file"
    name: "--input_scores"
    description: "A yaml file containing the scores of each of the methods"
    info: null
    example:
    - "score_uns.yaml"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--input_method_configs"
    info: null
    example:
    - "method_configs.yaml"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--input_metric_configs"
    info: null
    example:
    - "metric_configs.yaml"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--input_dataset_info"
    info: null
    example:
    - "dataset_info.yaml"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--input_execution"
    info: null
    example:
    - "trace.txt"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--input_task_info"
    info: null
    example:
    - "task_info.yaml"
    must_exist: true
    create_parent: true
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ";"
- name: "Outputs"
  arguments:
  - type: "file"
    name: "--output_scores"
    description: "A yaml file containing the scores of each of the methods"
    info: null
    default:
    - "results.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_method_info"
    info: null
    default:
    - "method_info.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_metric_info"
    info: null
    default:
    - "metric_info.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_dataset_info"
    info: null
    default:
    - "dataset_info.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_task_info"
    info: null
    default:
    - "task_info.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_qc"
    info: null
    default:
    - "quality_control.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
  - type: "file"
    name: "--output_metric_execution_info"
    info: null
    default:
    - "metric_execution_info.json"
    must_exist: true
    create_parent: true
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ";"
resources:
- type: "nextflow_script"
  path: "main.nf"
  is_executable: true
  entrypoint: "run_wf"
description: "This workflow transforms the meta information of the results into a\
  \ format that can be used by the website."
info: null
status: "enabled"
dependencies:
- name: "reporting/get_results"
  repository:
    type: "local"
- name: "reporting/get_method_info"
  repository:
    type: "local"
- name: "reporting/get_metric_info"
  repository:
    type: "local"
- name: "reporting/get_dataset_info"
  repository:
    type: "local"
- name: "reporting/get_task_info"
  repository:
    type: "local"
- name: "reporting/generate_qc"
  repository:
    type: "local"
license: "MIT"
links:
  repository: "https://github.com/openproblems-bio/openproblems"
  docker_registry: "ghcr.io"
runners:
- type: "nextflow"
  id: "nextflow"
  directives:
    tag: "$id"
  auto:
    simplifyInput: true
    simplifyOutput: false
    transcript: false
    publish: false
  config:
    labels:
      lowmem: "memory = 20.Gb"
      midmem: "memory = 50.Gb"
      highmem: "memory = 100.Gb"
      lowcpu: "cpus = 5"
      midcpu: "cpus = 15"
      highcpu: "cpus = 30"
      lowtime: "time = 1.h"
      midtime: "time = 4.h"
      hightime: "time = 8.h"
      veryhightime: "time = 24.h"
    script:
    - "process.errorStrategy = 'ignore'"
  debug: false
  container: "docker"
build_info:
  config: "src/reporting/process_task_results/config.vsh.yaml"
  runner: "nextflow"
  engine: "native"
  output: "target/nextflow/reporting/process_task_results"
  executable: "target/nextflow/reporting/process_task_results/main.nf"
  viash_version: "0.9.0"
  git_commit: "f0ee7b727ba6538a3480d54b5a47adae57fceff9"
  git_remote: "https://github.com/openproblems-bio/openproblems"
  git_tag: "v1.0.0-1421-gf0ee7b72"
  dependencies:
  - "target/nextflow/reporting/get_results"
  - "target/nextflow/reporting/get_method_info"
  - "target/nextflow/reporting/get_metric_info"
  - "target/nextflow/reporting/get_dataset_info"
  - "target/nextflow/reporting/get_task_info"
  - "target/nextflow/reporting/generate_qc"
package_config:
  name: "openproblems"
  version: "build_main"
  description: "Open Problems is a living, extensible, community-guided benchmarking\
    \ platform.\n"
  info:
    test_resources:
    - type: "s3"
      path: "s3://openproblems-data/resources_test/common"
      dest: "resources_test/common"
    - type: "s3"
      path: "s3://openproblems-data/resources_test/openproblems"
      dest: "resources_test/openproblems"
  viash_version: "0.9.0"
  source: "src"
  target: "target"
  config_mods:
  - ".runners[.type == \"nextflow\"].config.labels := { lowmem : \"memory = 20.Gb\"\
    , midmem : \"memory = 50.Gb\", highmem : \"memory = 100.Gb\", lowcpu : \"cpus\
    \ = 5\", midcpu : \"cpus = 15\", highcpu : \"cpus = 30\", lowtime : \"time = 1.h\"\
    , midtime : \"time = 4.h\", hightime : \"time = 8.h\", veryhightime : \"time =\
    \ 24.h\" }\n.runners[.type == \"nextflow\"].config.script := \"process.errorStrategy\
    \ = 'ignore'\"\n"
  keywords:
  - "openproblems"
  - "benchmarking"
  - "single-cell omics"
  license: "MIT"
  organization: "openproblems-bio"
  references:
    doi:
    - "10.21203/rs.3.rs-4181617/v1"
  links:
    repository: "https://github.com/openproblems-bio/openproblems"
    docker_registry: "ghcr.io"
    issue_tracker: "https://github.com/openproblems-bio/openproblems/issues"
