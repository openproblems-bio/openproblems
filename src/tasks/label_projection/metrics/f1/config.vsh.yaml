__merge__: ../../api/comp_metric.yaml
functionality:
  name: "f1"
  info:
    metrics:
      - name: f1_weighted
        label: F1 weighted
        summary: "Average weigthed support between each labels F1 score"
        description: "Calculates the F1 score for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall."
        reference: grandini2020metrics
        min: 0
        max: 1
        maximize: true
        v1:
          path: openproblems/tasks/label_projection/metrics/f1.py
          commit: b3456fd73c04c28516f6df34c57e6e3e8b0dab32
      - name: f1_macro
        label: F1 macro
        summary: "Unweighted mean of each label F1-score"
        description: "Calculates the F1 score for each label, and find their unweighted mean. This does not take label imbalance into account."
        reference: grandini2020metrics
        min: 0
        max: 1
        maximize: true
        v1:
          path: openproblems/tasks/label_projection/metrics/f1.py
          commit: b3456fd73c04c28516f6df34c57e6e3e8b0dab32
      - name: f1_micro
        label: F1 micro
        summary: "Calculation of TP, FN and FP."
        description: "Calculates the F1 score globally by counting the total true positives, false negatives and false positives."
        reference: grandini2020metrics
        min: 0
        max: 1
        maximize: true
        v1:
          path: openproblems/tasks/label_projection/metrics/f1.py
          commit: b3456fd73c04c28516f6df34c57e6e3e8b0dab32
  resources:
    - type: python_script
      path: script.py
platforms:
  - type: docker
    image: openproblems/base_python:1.0.0
    setup:
      - type: python
        packages: scikit-learn
  - type: nextflow
    directives:
      label: [midtime, midmem, midcpu]
